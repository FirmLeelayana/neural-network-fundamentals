# -*- coding: utf-8 -*-
"""NeuralNetworks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orXM29tiaQTVcPCglEpeIefssdXFoj5f
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston,load_breast_cancer

X, y = load_boston(return_X_y = True)

#reshape to make sure dimensions are right
X = X.T
y = y[None, :]
# n examples (1 ful col = 1 example) and m feature (rows)

#normalise data
X = (X - X.mean(1, keepdims = True))/X.std(1, keepdims=True)

#split data into train and train set
X_train = X[:, :400]
Y_train = y[:, :400]

X_test = X[:, 400:]
Y_test = y[:, 400:]

def initialise_parameters(layers_units):
    parameters = {}
    for l in range(1, len(layers_units)):
        parameters['W' + str(l)] = np.random.randn(layers_units[l], layers_units[l-1])*0.001
        parameters['b' + str(l)] = np.random.rand(layers_units[l], 1)*0.001
    return parameters

def sigmoid(z, deriv = False):
    if (deriv):
        return 1./(1. + np.exp(-z))
    else:
        return np.exp(-z)/(1. + np.exp(-z))**2

def relu(z, deriv = False):
    if (deriv): #This is for gradients
        return 1. if z>0 else 0
    else:
        return np.maximum(z, 0)

def forward_propagation(X, parameters, g=relu):
    cache = {} #stores all intermediate calculations
    L = len(parameters)//2 #no of layers
    cache["A0"] = X
    for l in range(1, L):
        cache['Z' + str(l)] = parameters['W' + str(l)] @ cache['A'+str(l-1)] + parameters['b' + str(l)]
        cache['A' + str(l)] = g(cache['Z' + str(l)])
    
    #final layer
    cache['Z' + str(L)] = parameters['W' + str(L)] @ cache['A' + str(L-1)] + parameters['b'+str(L)]
    cache['A' + str(L)] = cache['Z' + str(L)] #note no act. function here
    return cache

def cost_function(A_L, Y): #A_L is the value of A for the last layer
    '''
    calculates cost function 
    '''
    
    m = Y.shape[1] #no of examples
    cost = 1./(2*m) * ((A_L- Y) ** 2).sum() 
    return cost

def backpropagation(cache, Y, parameters, g= relu): #backprop is partial derivatives and chain rule
    L=len(parameters) //2
    m = Y.shape[1]
    grads = {}
    # loop up to the last layer, do last layer explicitly
    grads["dZ" + str(L)] = cache['A' + str(L)] - Y
    grads['dW' + str(L)] = 1./m * grads['dZ' + str(L)] @ cache['A' + str(L-1)].T
    grads['db' + str(L)] = grads["dZ" + str(L)].mean(1)
    for l in range(L-1,0,-1): #going backwards from final layer to first layer
        grads['dA' + str(l)] = parameters['W' + str(l+1)].T @ grads["dZ" + str(l+1)]
        grads['dZ' + str(l)] = grads['dA' + str(l)] @ g(cache['Z' + str(l)], deriv = True)
        grads['dW' + str(l)] = 1./m * grads['dZ' + str(l)] @cache['A' + str(l-1)].T
        grads['db' + str(l)] = grads['dZ' + str(l)].mean(1)
    return grads

def train_model(X_train, Y_train, X_test, Y_test, num_epochs, layers_units, learning_rate):
    '''
    this is a docstring
    '''
    train_costs = []
    test_costs = []

    #initialize the parameters, recall the initialise parameters function created earlier
    parameters = initialise_parameters(layers_units)

    L = len(layers_units) -1
    for epoch in range(num_epochs):
        #run one step of forward prop
        train_cache = forward_propagation(X_train, parameters)
        train_A_L = train_cache['A' + str(L)]

        #cal cost
        cost = cost_function(train_A_L, Y_train)

        grads = backpropagation(train_cache, Y_train, parameters)

        #iterate through each layer and update parameters using gradient descent
        for l in range(1, L+1): # recall parameters is a dictionary
            parameters["W" + str(l)] -= learning_rate*grads["dW" + str(l)]
            parameters['b' + str(l)] -= learning_rate*grads['db' + str(l)]

        #periodically output update on current cost and performance for visualisation
        train_costs.append(cost)

        test_cache = forward_propagation(X_test, parameters)
        test_A_L = test_cache['A' + str(L)]

        cost_test = cost_function(test_A_L, Y_test)
        test_costs.append(cost_test)

        if (epoch % (num_epochs//10)==0):
            print('Training the model, epoch: ' + str(epoch +1))
            print('cost after epoch ' + str((epoch)) + ': ' + str(cost))
    print('Training complete!')

    return parameters, train_costs, test_costs

def evaluate_model(train_costs, test_costs, parameters, X_train, Y_train, X_test, Y_test):

    #plot the graphs of training set error
    plt.plot(np.squeeze(train_costs))
    plt.plot(np.squeeze(test_costs),'r--')
    plt.ylabel('Cost')
    plt.xlabel('iterations')
    plt.title('Training set error')
    plt.show()
    

    L= len(parameters)//2

    #obtain the trained models predictions and evaluate this
    train_cache = forward_propagation(X_train, parameters)
    train_A_L = train_cache['A' + str(L)]

    print('The train set MSE is : ' + str(cost_function(train_A_L, Y_train)))

    test_cache = forward_propagation(X_test, parameters)
    test_A_L = test_cache["A" + str(L)]

    print('The train set MSE is : ' + str(cost_function(test_A_L, Y_test)))

num_epochs = 150000
layers_units = [X.shape[0], 1] # layer 0 is the input layer - each value in list = number of nodes in that layer
learning_rate = 1e-4

#1 hidden layer with 1 node

parameters, train_costs, test_costs = train_model(X_train, Y_train, X_test, Y_test, num_epochs, layers_units, learning_rate)

evaluate_model(train_costs, test_costs, parameters, X_train, Y_train, X_test, Y_test )

